{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Delayed Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Naive markovian policies\n",
    "\n",
    "To attempt writing fixed-point equations for $V^\\pi(s)$ under the given delayed MDP with naive Markovian policies, we first consider the standard Bellman equation in a typical MDP without delays:\n",
    "\n",
    "\n",
    "$$ V^\\pi(s) = r(s, \\pi(s)) + \\lambda \\sum_{s{\\prime}} p(s{\\prime} \\mid s, \\pi(s)) V^\\pi(s{\\prime})$$\n",
    "\n",
    "In this equation, the value at state $s$ depends directly on the immediate reward received at $s$ and the expected future value $V^\\pi(s{\\prime})$ of the next state $s{\\prime}$, given action $\\pi(s)$.\n",
    "\n",
    "In the delayed MDP scenario at hand, the agent’s action at time $t$ depends on the state from $d$ steps ago, $s_{t-d}$. Specifically, for $t \\geq d+1$, the action is $a_t = \\pi(s_{t-d})$. This leads to the following problems when trying to write a fixed-point equation for $V^\\pi(s)$: \n",
    "\n",
    "1. The standard Bellman equation requires the future to be independent of the past given the present state and action, as defined in the lecture on the board or more formal on slide 11 from the first slide deck in the Markovian dynamics. In the given case the action at time $t$ depends on $s_{t-d}$, a past state, rather than the current state $s_t$, thus making the process non-Markovian with respect to $s_t$.\n",
    "\n",
    "2. Due to the delayed dependence, the immediate reward $r(s_t, a_t)$ and the transition probabilities $p(s_{t+1} \\mid s_t, a_t)$ depend on $s_{t-d}$ through $a_t$. This creates a situation where  V^\\pi(s)  cannot be expressed solely in terms of  V^\\pi(s{\\prime})  for states  s{\\prime}  reachable from  s  in one step. The recursive structure, as introduced on slide 24 part 1, required for solving a fixed-point equation is broken.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Equivalent augmented state MDP\n",
    "\n",
    "Agumenting the states with $\\bar{s}_t =(s_{t−d},a_{t−d},...,a_{t−1})$ reduces the problem to an MDP with no delay. By augmenting the states this way the MDP becomes Markovian with respect to the augmented state $\\bar{s}_t$, thus solving the problems from 2.1. The additional states \"absorb\" the delays and makes it possible for the agent to operate in an MDP without delays, where standard reinforcement learning techniques can be applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At time t, the augmented state is:\n",
    "\n",
    "$\\bar{s}_t = (s_{t-d}, a_{t-d}, a_{t-d+1}, \\ldots, a_{t-1})$\n",
    "\n",
    "- $s_{t-d}$: The state observed d steps ago (the most recent state information available).\n",
    "- $(a_{t-d}, \\ldots, a_{t-1})$: The sequence of actions taken since observing $s_{t-d}$.\n",
    "\n",
    "By including these elements, the augmented state  $\\bar{s}_t$  contains all the necessary historical information to determine future transitions and rewards based solely on $\\bar{s}_t$ and the current action $a_t$. \n",
    "\n",
    "When the agent takes action $a_t$ at time $t$, the next augmented state $\\bar{s}_{t+1}$ becomes:\n",
    "\n",
    "$\\bar{s}_{t+1} = (s_{t+1 - d}, a_{t+1 - d}, \\ldots, a_t)$\n",
    "\n",
    "\n",
    "The transition probability $P(\\bar{s}_{t+1} \\mid \\bar{s}_t, a_t)$ is determined by:\n",
    "1. State component update: $s_{t-d} = s_{t+1 - d}$ with transition probability: \n",
    "$P(s_{t - d + 1} \\mid s_{t - d}, a_{t - d})$\n",
    "- This comes from the original MDP’s state transition probabilities and we assume that it is given \n",
    "2. Action Components Update: $(a_{t+1 - d}, \\ldots, a_t) = (a_{t - d + 1}, \\ldots, a_t)$\n",
    "- Deterministic Update: The actions  $a_{t - d + 1}, \\ldots, a_t$ are known from previous decisions.\n",
    "\n",
    "### Constructing the Transition Probability:\n",
    "\n",
    "The transition probability from  $\\bar{s}_t$ to $\\bar{s}_{t+1}$ given action $a_t$ is:\n",
    "\n",
    "\n",
    "$$P(\\bar{s}{t+1} \\mid \\bar{s}t, a_t) = P(s{t - d + 1} \\mid s{t - d}, a_{t - d}) \\times I\\left(\\text{Action components updated as expected} \\right)$$\n",
    "\n",
    "- $I(\\cdot)$ is the indicator function, which equals 1 if the action components in $\\bar{s}_{t+1}$ are updated exactly as per the deterministic rules, and 0 otherwise.\n",
    "\n",
    "Since the action components always update deterministically (they are known and controlled by the agent), the indicator function $I(\\cdot)$ is always 1 when considering valid transitions. Therefore, the transition probability simplifies to:\n",
    "\n",
    "$$P(\\bar{s}_{t+1} \\mid \\bar{s}_t, a_t) = P(s_{t - d + 1} \\mid s_{t - d}, a_{t - d}) \\times 1$$\n",
    "\n",
    "This expression holds only for the valid transitions where action components update as expected; otherwise, the transition probability is zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function \n",
    "The reward function in the augmented MDP is:\n",
    "\n",
    "$r(\\bar{s}_t, a_t) = r(s_{t-d}, a_{t-d})$\n",
    "\n",
    "The immediate reward at time t depends on the delayed state $s_{t-d}$ and delayed action $a_{t-d}$, both components of $\\bar{s}_t$.\n",
    "The reward does not depend on the current action $a_t$ because the environment provides feedback based on actions taken d steps ago.\n",
    "Therefore the reward at time $t$ corresponds to the action taken at time $t-d$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: In the equivalent MDP, consider a deterministic policy $\\pi$ defined by $\\pi(\\bar{s})$ for all augmented state $\\bar{s}$. Write the fixed point equations satisfied by $V^\\pi$. Propose a RL algorithm to solve them.\n",
    "\n",
    "Given the deterministic policy $\\pi$ the value function $V^\\pi(\\bar{s})$ satisfies the following fixed-point equation (the Bellman equation for policy evaluation):\n",
    "$$\n",
    "V^\\pi(\\bar{s}) = r(\\bar{s}, \\pi(\\bar{s})) + \\gamma \\sum_{\\bar{s}{\\prime}} P(\\bar{s}{\\prime} \\mid \\bar{s}, \\pi(\\bar{s})) V^\\pi(\\bar{s}{\\prime})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\bar{s}$ is the current augmented state.\n",
    "- $\\pi(\\bar{s})$ is the action taken in state  \\bar{s}  according to the policy.\n",
    "- $r(\\bar{s}, \\pi(\\bar{s}))$ is the immediate reward received when taking action $\\pi(\\bar{s})$ in state $\\bar{s}$.\n",
    "- $\\gamma$ is the discount factor (with $0 \\leq \\gamma < 1$).\n",
    "- $P(\\bar{s}{\\prime} \\mid \\bar{s}, \\pi(\\bar{s}))$ is the transition probability from $\\bar{s}$ to $\\bar{s}{\\prime}$ when action $\\pi(\\bar{s})$ is taken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this fixed point equation by using a value iteration algorithm. This is possible because we have a complete model of the environment, as we know the stationary transition probabilities $p(\\cdot | s, a)$ and the reward function according to the exercise.\n",
    "\n",
    "The outline of the algorithm is as followed: \n",
    "\n",
    "- Start by initializing $V(\\bar{s})$ for all states to some arbitrary values (e.g., zeros).\n",
    "- Update $V(\\bar{s})$ for each state based on the Bellman update equation:\n",
    "\n",
    "$V(\\bar{s}) = \\max_a \\left[ r(\\bar{s}, a) + \\gamma \\sum_{\\bar{s}{\\prime}} P(\\bar{s}{\\prime} \\mid \\bar{s}, a) V(\\bar{s}{\\prime}) \\right]$\n",
    "\n",
    "- The loop continues until the maximum change in $V(\\bar{s})$ between iterations, $\\Delta$, is less than a small threshold $\\theta$. \n",
    "- After the value function converges, we can extract an optimal policy $\\pi^*$ by selecting actions that maximize the expected value at each state.\n",
    "\n",
    "In the below pseudo code the following variables are used: \n",
    "- $\\gamma$ is the discount factor, which determines how future rewards are valued relative to immediate rewards.\n",
    "- $P(\\bar{s}{\\prime} \\mid \\bar{s}, a)$ represents the transition probability from state $\\bar{s}$ to $\\bar{s}{\\prime}$ given action $a$.\n",
    "- $r(\\bar{s}, a)$ is the immediate reward received after taking action $a$ in state $\\bar{s}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "// Initialize V(𝑠̅) arbitrarily for all augmented states 𝑠̅\n",
    "V = {}\n",
    "for 𝑠̅ in all_possible_augmented_states:\n",
    "    V[𝑠̅] = 0\n",
    "\n",
    "// Set a threshold for convergence\n",
    "θ = 0.0001  // A small positive number\n",
    "\n",
    "// Value Iteration Loop:\n",
    "while True:\n",
    "    Δ = 0  // Initialize the maximum change in V to zero\n",
    "    for 𝑠̅ in all_possible_augmented_states:\n",
    "        v = V[𝑠̅]  // Store the current value of the state\n",
    "\n",
    "        // Calculate the new value using the Bellman update equation\n",
    "        V[𝑠̅] = max_a [r(𝑠̅, a) + γ * sum_{𝑠̅'} P(𝑠̅' | 𝑠̅, a) * V[𝑠̅']]\n",
    "\n",
    "        // Update the maximum change in V\n",
    "        Δ = max(Δ, abs(v - V[𝑠̅]))\n",
    "\n",
    "    // Check for convergence\n",
    "    if Δ < θ:\n",
    "        break  // Exit the loop when the change is below the threshold\n",
    "\n",
    "// Policy Extraction (optional, to derive an optimal policy π* from V)\n",
    "π = {}\n",
    "for 𝑠̅ in all_possible_augmented_states:\n",
    "    // Select the action that maximizes the expected value\n",
    "    π[𝑠̅] = argmax_a [r(𝑠̅, a) + γ * sum_{𝑠̅'} P(𝑠̅' | 𝑠̅, a) * V[𝑠̅']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Q-learning algorithm would also be an option to answer this question. However, it does not directly solve for the value function $V^\\pi(\\bar{s})$ as defined by the Bellman equation stated above. This is because Q-learning is an off-policy method that estimates the optimal action-value function $Q^*(\\bar{s}, a)$. It updates the Q-values using the observed rewards and future estimated values but does not explicitly calculate $V^\\pi(\\bar{s})$. From the output of the Q algorithm we can infer the value function. We would use this method if the transition probabilities were unknown. \n",
    "\n",
    "```\n",
    "// Initialize Q(𝑠̅, a) arbitrarily for all 𝑠̅ and a\n",
    "Q = {} \n",
    "for s in all_possible_augmented_states:\n",
    "    for a in all_possible_actions:\n",
    "        Q[(s, a)] = 0 \n",
    "\n",
    "// Episode Loop for e episodes:\n",
    "For i in range(0, e):\n",
    "    // Initialize the augmented state 𝑠̅ with dummy values\n",
    "    𝑠̅ = (0, 0, ..., 0) // Initialize 𝑠̅ with dummy state and action history\n",
    "\n",
    "    // Time Step Loop:\n",
    "    While not end of episode:\n",
    "        // 1. Action Selection:\n",
    "        If random() < ε:\n",
    "            aₜ = random_action()\n",
    "        else:\n",
    "            aₜ = argmax_a Q(𝑠̅, a)\n",
    "\n",
    "        // 2. Environment Interaction:\n",
    "        // Execute action aₜ\n",
    "        r = r(sₜ₋d, aₜ₋d) // Observe reward based on delayed state and action\n",
    "        s' = sample_next_state(sₜ₋d, aₜ₋d) // Observe next state based on transition probability\n",
    "\n",
    "        // Update the augmented state:\n",
    "        𝑠̅' = (s', aₜ₋(d-1), ..., aₜ₋₁, aₜ)\n",
    "\n",
    "        // 3. Q-value Update:\n",
    "        Q(𝑠̅, aₜ) = Q(𝑠̅, aₜ) + α * [r + λ * max_{a'} Q(𝑠̅', a') - Q(𝑠̅, aₜ)]\n",
    "\n",
    "        // 4. State Transition:\n",
    "        𝑠̅ = 𝑠̅'\n",
    "\n",
    "    // Episode Termination:\n",
    "    // End the episode after a fixed number of steps or when a terminal condition is met\n",
    "// After running Q-learning, infer the optimal value function V*(𝑠̅)\n",
    "For s in all_possible_augmented_states:\n",
    "    V*(s) = max_a Q(s, a)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
