{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.13/site-packages (2.1.3)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in ./.venv/lib/python3.13/site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.13/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in ./.venv/lib/python3.13/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.13/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: IPython in ./.venv/lib/python3.13/site-packages (8.29.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.13/site-packages (from IPython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.13/site-packages (from IPython) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.13/site-packages (from IPython) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.13/site-packages (from IPython) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.13/site-packages (from IPython) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./.venv/lib/python3.13/site-packages (from IPython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in ./.venv/lib/python3.13/site-packages (from IPython) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.13/site-packages (from IPython) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./.venv/lib/python3.13/site-packages (from jedi>=0.16->IPython) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.13/site-packages (from pexpect>4.3->IPython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.13/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.13/site-packages (from stack-data->IPython) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.13/site-packages (from stack-data->IPython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.13/site-packages (from stack-data->IPython) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.13/site-packages (from asttokens>=2.1.0->stack-data->IPython) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython import display\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = ['DynProg', 'ValIter']\n",
    "\n",
    "# Some colours\n",
    "LIGHT_RED    = '#FFC4CC'\n",
    "LIGHT_GREEN  = '#95FD99'\n",
    "BLACK        = '#000000'\n",
    "WHITE        = '#FFFFFF'\n",
    "LIGHT_PURPLE = '#E8D0FF'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "\n",
    "    # Actions\n",
    "    STAY       = 0\n",
    "    MOVE_LEFT  = 1\n",
    "    MOVE_RIGHT = 2\n",
    "    MOVE_UP    = 3\n",
    "    MOVE_DOWN  = 4\n",
    "\n",
    "    # Give names to actions\n",
    "    actions_names = {\n",
    "        STAY: \"stay\",\n",
    "        MOVE_LEFT: \"move left\",\n",
    "        MOVE_RIGHT: \"move right\",\n",
    "        MOVE_UP: \"move up\",\n",
    "        MOVE_DOWN: \"move down\"\n",
    "    }\n",
    "\n",
    "    # Reward values \n",
    "    STEP_REWARD = -0.01         #small penality for every step to incourage finding the exit faster (or being eaten)\n",
    "    GOAL_REWARD = 100         #possitive reward for exiting the maze\n",
    "    IMPOSSIBLE_REWARD = -1   #penalizing impossible moves\n",
    "    MINOTAUR_REWARD = -10     #TODO\n",
    "\n",
    "    def __init__(self, maze):\n",
    "        \"\"\" Constructor of the environment Maze.\n",
    "        \"\"\"\n",
    "        self.maze                     = maze\n",
    "        self.actions                  = self.__actions()\n",
    "        self.states, self.map         = self.__states()\n",
    "        self.n_actions                = len(self.actions)\n",
    "        self.n_states                 = len(self.states)\n",
    "        print(self.n_states)\n",
    "        self.transition_probabilities = self.__transitions()\n",
    "        self.rewards                  = self.__rewards()\n",
    "\n",
    "    def __actions(self):\n",
    "        actions = dict()\n",
    "        actions[self.STAY]       = (0, 0)\n",
    "        actions[self.MOVE_LEFT]  = (0,-1)\n",
    "        actions[self.MOVE_RIGHT] = (0, 1)\n",
    "        actions[self.MOVE_UP]    = (-1,0)\n",
    "        actions[self.MOVE_DOWN]  = (1,0)\n",
    "        return actions\n",
    "\n",
    "    def __states(self):\n",
    "        \n",
    "        states = dict()\n",
    "        map = dict()\n",
    "        s = 0\n",
    "        for i in range(self.maze.shape[0]):\n",
    "            for j in range(self.maze.shape[1]):\n",
    "                for k in range(self.maze.shape[0]):\n",
    "                    for l in range(self.maze.shape[1]):\n",
    "                        if self.maze[i,j] != 1:\n",
    "                            states[s] = ((i,j), (k,l))\n",
    "                            map[((i,j), (k,l))] = s\n",
    "                            s += 1\n",
    "        \n",
    "        states[s] = 'Eaten'\n",
    "        map['Eaten'] = s\n",
    "        s += 1\n",
    "        \n",
    "        states[s] = 'Win'\n",
    "        map['Win'] = s\n",
    "        return states, map\n",
    "\n",
    "    def move(self, state, action):               \n",
    "        \"\"\" Makes a step in the maze, given a current position and an action. \n",
    "            If the action STAY or an inadmissible action is used, the player stays in place.\n",
    "        \n",
    "            :return list of tuples next_state: Possible states ((x,y), (x',y')) on the maze that the system can transition to.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.states[state] == 'Eaten' or self.states[state] == 'Win': # In these states, the game is over\n",
    "            return [self.states[state]]\n",
    "        \n",
    "        else: # Compute the future possible positions given current (state, action)\n",
    "            row_player = self.states[state][0][0] + self.actions[action][0] # Row of the player's next position \n",
    "            col_player = self.states[state][0][1] + self.actions[action][1] # Column of the player's next position \n",
    "            \n",
    "            # Is the player getting out of the limits of the maze or hitting a wall?\n",
    "            \n",
    "            impossible_action_player = (\n",
    "                row_player < 0 or row_player >= self.maze.shape[0] or\n",
    "                col_player < 0 or col_player >= self.maze.shape[1] or\n",
    "                self.maze[row_player, col_player] == 1  # Check if the position is a wall\n",
    "            )\n",
    "            actions_minotaur = [[0, -1], [0, 1], [-1, 0], [1, 0]] # Possible moves for the Minotaur\n",
    "            rows_minotaur, cols_minotaur = [], []\n",
    "            for i in range(len(actions_minotaur)):\n",
    "                # Is the minotaur getting out of the limits of the maze?\n",
    "                impossible_action_minotaur = (self.states[state][1][0] + actions_minotaur[i][0] == -1) or \\\n",
    "                                             (self.states[state][1][0] + actions_minotaur[i][0] == self.maze.shape[0]) or \\\n",
    "                                             (self.states[state][1][1] + actions_minotaur[i][1] == -1) or \\\n",
    "                                             (self.states[state][1][1] + actions_minotaur[i][1] == self.maze.shape[1])\n",
    "            \n",
    "                if not impossible_action_minotaur:\n",
    "                    rows_minotaur.append(self.states[state][1][0] + actions_minotaur[i][0])\n",
    "                    cols_minotaur.append(self.states[state][1][1] + actions_minotaur[i][1])  \n",
    "          \n",
    "\n",
    "            # Based on the impossiblity check return the next possible states.\n",
    "            if impossible_action_player:  # The action is not possible, so the player remains in place\n",
    "                states = []\n",
    "                for i in range(len(rows_minotaur)):\n",
    "                    if (self.states[state][0][0], self.states[state][0][1]) == (rows_minotaur[i], cols_minotaur[i]):\n",
    "                        # The player is caught by the minotaur\n",
    "                        states.append('Eaten')\n",
    "                    elif (self.states[state][0][0], self.states[state][0][1]) == (np.where(self.maze == 2)[0][0], np.where(self.maze == 2)[1][0]):\n",
    "                        # The player is at the exit state, without meeting the minotaur\n",
    "                        states.append('Win')\n",
    "                    else:\n",
    "                        # The player remains in place, and the minotaur moves randomly\n",
    "                        states.append(((self.states[state][0][0], self.states[state][0][1]), (rows_minotaur[i], cols_minotaur[i])))\n",
    "                return states\n",
    "          \n",
    "            else:  # The action is possible, the player moves to the new position\n",
    "                states = []\n",
    "                for i in range(len(rows_minotaur)):\n",
    "                    if (row_player, col_player) == (rows_minotaur[i], cols_minotaur[i]):\n",
    "                        # The player is caught by the minotaur\n",
    "                        states.append('Eaten')\n",
    "                    elif (row_player, col_player) == (np.where(self.maze == 2)[0][0], np.where(self.maze == 2)[1][0]):\n",
    "                        # The player reaches the exit without being caught\n",
    "                        states.append('Win')\n",
    "                    else:\n",
    "                        # The player moves to the new position, and the minotaur moves randomly\n",
    "                        states.append(((row_player, col_player), (rows_minotaur[i], cols_minotaur[i])))\n",
    "                return states\n",
    "        \n",
    "        \n",
    "\n",
    "    def __transitions(self):\n",
    "        \"\"\" Computes the transition probabilities for every state action pair.\n",
    "            :return numpy.tensor transition probabilities: tensor of transition\n",
    "            probabilities of dimension S*S*A\n",
    "        \"\"\"\n",
    "        # Initialize the transition probailities tensor (S,S,A)\n",
    "        dimensions = (self.n_states,self.n_states,self.n_actions)\n",
    "        transition_probabilities = np.zeros(dimensions)\n",
    "\n",
    "        # TODO: Compute the transition probabilities.\n",
    "  \n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                # Check if the current state is terminal\n",
    "                if self.states[s] == 'Eaten' or self.states[s] == 'Win':\n",
    "                    # A terminal state should transition to itself with probability 1\n",
    "                    transition_probabilities[s, s, a] = 1.0\n",
    "                    continue\n",
    "\n",
    "                # Get the possible next states when taking action `a` from state `s`\n",
    "                next_states = self.move(s, a)\n",
    "                num_next_states = len(next_states)\n",
    "\n",
    "                # Uniform probability for each possible next state\n",
    "                prob = 1 / num_next_states if num_next_states > 0 else 0\n",
    "\n",
    "                for next_state in next_states:\n",
    "                    next_s = self.map[next_state]\n",
    "                    transition_probabilities[s, next_s, a] = prob\n",
    "\n",
    "        return transition_probabilities\n",
    "\n",
    "\n",
    "\n",
    "    def __rewards(self):\n",
    "        \n",
    "        \"\"\" Computes the rewards for every state action pair \"\"\"\n",
    "\n",
    "        rewards = np.zeros((self.n_states, self.n_actions))\n",
    "        \n",
    "        for s in range(self.n_states):\n",
    "            for a in range(self.n_actions):\n",
    "                \n",
    "                if self.states[s] == 'Eaten': # The player has been eaten\n",
    "                    rewards[s, a] = self.MINOTAUR_REWARD\n",
    "                \n",
    "                elif self.states[s] == 'Win': # The player has won\n",
    "                    rewards[s, a] = self.GOAL_REWARD\n",
    "                \n",
    "                else:                \n",
    "                    next_states = self.move(s,a)\n",
    "                    next_s = next_states[0] # The reward does not depend on the next position of the minotaur, we just consider the first one\n",
    "                    \n",
    "                    if self.states[s][0] == next_s[0] and a != self.STAY: # The player hits a wall\n",
    "                        rewards[s, a] = self.IMPOSSIBLE_REWARD\n",
    "                    \n",
    "                    else: # Regular move\n",
    "                        rewards[s, a] = self.STEP_REWARD\n",
    "\n",
    "        return rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def simulate(self, start, policy, method):\n",
    "        \n",
    "        if method not in methods:\n",
    "            error = 'ERROR: the argument method must be in {}'.format(methods)\n",
    "            raise NameError(error)\n",
    "\n",
    "        path = list()\n",
    "        \n",
    "        if method == 'DynProg':\n",
    "            horizon = policy.shape[1] # Deduce the horizon from the policy shape\n",
    "            t = 0 # Initialize current time\n",
    "            s = self.map[start] # Initialize current state \n",
    "            path.append(start) # Add the starting position in the maze to the path\n",
    "            \n",
    "            while t < horizon - 1:\n",
    "                a = policy[s, t] # Move to next state given the policy and the current state\n",
    "                next_states = self.move(s, a) \n",
    "                if next_states[0] in ['Eaten', 'Win']:\n",
    "                    path.append(next_states[0])  # Append the terminal state name\n",
    "                    print(\"Terminal State: \", next_states[0])\n",
    "                    break  # Stop the simulation if a terminal state is reached\n",
    "                next_s = next_states[0]\n",
    "                path.append(next_s) # Add the next state to the path\n",
    "                t +=1 # Update time and state for next iteration\n",
    "                s = self.map[next_s] \n",
    "            print(\"did not win nor loose\") \n",
    "                          \n",
    "        if method == 'ValIter': \n",
    "            t = 1 # Initialize current state, next state and time\n",
    "            s = self.map[start]\n",
    "            path.append(start) # Add the starting position in the maze to the path\n",
    "            next_states = self.__move(s, policy[s]) # Move to next state given the policy and the current state\n",
    "            #next_s = \n",
    "            path.append(next_s) # Add the next state to the path\n",
    "            \n",
    "            #horizon =                               # Question e\n",
    "            # Loop while state is not the goal state\n",
    "            while s != next_s and t <= horizon:\n",
    "                s = self.map[next_s] # Update state\n",
    "                next_states = self.__move(s, policy[s]) # Move to next state given the policy and the current state\n",
    "                #next_s = \n",
    "                path.append(next_s) # Add the next state to the path\n",
    "                t += 1 # Update time for next iteration\n",
    "        \n",
    "        return [path, horizon] # Return the horizon as well, to plot the histograms for the VI\n",
    "\n",
    "\n",
    "\n",
    "    def show(self):\n",
    "        print('The states are :')\n",
    "        print(self.states)\n",
    "        print('The actions are:')\n",
    "        print(self.actions)\n",
    "        print('The mapping of the states:')\n",
    "        print(self.map)\n",
    "        print('The rewards:')\n",
    "        print(self.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_programming(env, horizon):\n",
    "    \"\"\" Solves the shortest path problem using dynamic programming\n",
    "        :input Maze env           : The maze environment in which we seek to\n",
    "                                    find the shortest path.\n",
    "        :input int horizon        : The time T up to which we solve the problem.\n",
    "        :return numpy.array V     : Optimal values for every state at every\n",
    "                                    time, dimension S*T\n",
    "        :return numpy.array policy: Optimal time-varying policy at every state,\n",
    "                                    dimension S*T\n",
    "    \"\"\"\n",
    "    # Intializing value function V and policy as a 2D array with the number of possible states and the time horizon t\n",
    "    V = np.zeros((env.n_states, horizon + 1))\n",
    "    policy = np.zeros((env.n_states, horizon), dtype=int)\n",
    "    # Backward induction over time steps\n",
    "    for t in range(horizon - 1, -1, -1):\n",
    "        for s in range(env.n_states):\n",
    "            # Skip terminal states\n",
    "            if env.states[s] == 'Eaten' or env.states[s] == 'Win':\n",
    "                continue\n",
    "\n",
    "            best_value = float('-inf')\n",
    "            best_action = None\n",
    "\n",
    "            # Evaluate each action\n",
    "            for a in range(env.n_actions):\n",
    "                next_states = env.move(s, a)\n",
    "                expected_value = 0\n",
    "\n",
    "                for next_state in next_states:\n",
    "                    next_s = env.map[next_state]\n",
    "                    # Calculate transition probability\n",
    "                    transition_prob = env.transition_probabilities[s, next_s, a]\n",
    "                    # Sum the expected value\n",
    "                    expected_value += transition_prob * V[next_s, t + 1]\n",
    "\n",
    "                # Include the reward for the current (s, a) pair\n",
    "                expected_value += env.rewards[s, a]\n",
    "\n",
    "                # Update the best action if this one is better\n",
    "                if expected_value > best_value:\n",
    "                    best_value = expected_value\n",
    "                    best_action = a\n",
    "\n",
    "            # Update the value function and policy\n",
    "            V[s, t] = best_value\n",
    "            policy[s, t] = best_action\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_transition_probabilities(env):\n",
    "    for s in range(env.n_states):\n",
    "        if env.states[s] not in ['Eaten', 'Win']:  # Skip terminal states\n",
    "            for a in range(env.n_actions):\n",
    "                total_prob = np.sum(env.transition_probabilities[s, :, a])\n",
    "                #if not np.isclose(total_prob, 1.0):\n",
    "                    #print(f\"Warning: Transition probabilities for state {s} and action {a} do not sum to 1. Total = {total_prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma, epsilon):\n",
    "    \"\"\" Solves the shortest path problem using value iteration\n",
    "        :input Maze env           : The maze environment in which we seek to\n",
    "                                    find the shortest path.\n",
    "        :input float gamma        : The discount factor.\n",
    "        :input float epsilon      : accuracy of the value iteration procedure.\n",
    "        :return numpy.array V     : Optimal values for every state at every\n",
    "                                    time, dimension S*T\n",
    "        :return numpy.array policy: Optimal time-varying policy at every state,\n",
    "                                    dimension S*T\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return V, policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_solution(maze, path):\n",
    "\n",
    "    # Map a color to each cell in the maze\n",
    "    col_map = {0: WHITE, 1: BLACK, 2: LIGHT_GREEN, -1: LIGHT_RED, -2: LIGHT_PURPLE}\n",
    "    \n",
    "    rows, cols = maze.shape # Size of the maze\n",
    "    fig = plt.figure(1, figsize=(cols, rows)) # Create figure of the size of the maze\n",
    "\n",
    "    # Remove the axis ticks and add title\n",
    "    ax = plt.gca()\n",
    "    ax.set_title('Policy simulation')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Give a color to each cell\n",
    "    colored_maze = [[col_map[maze[j, i]] for i in range(cols)] for j in range(rows)]\n",
    "\n",
    "    # Create a table to color\n",
    "    grid = plt.table(\n",
    "        cellText = None, \n",
    "        cellColours = colored_maze, \n",
    "        cellLoc = 'center', \n",
    "        loc = (0,0), \n",
    "        edges = 'closed'\n",
    "    )\n",
    "    \n",
    "    # Modify the height and width of the cells in the table\n",
    "    tc = grid.properties()['children']\n",
    "    for cell in tc:\n",
    "        cell.set_height(1.0/rows)\n",
    "        cell.set_width(1.0/cols)\n",
    "    print(path)\n",
    "    for i in range(0, len(path)):\n",
    "        if path[i-1] != 'Eaten' and path[i-1] != 'Win':\n",
    "            print(path)\n",
    "            print(path[i-1])\n",
    "            print(path[i-1][0])\n",
    "            print(maze[path[i-1][0]])\n",
    "            print(col_map)\n",
    "            grid.get_celld()[(path[i-1][0])].set_facecolor(col_map[maze[path[i-1][0]]])\n",
    "            grid.get_celld()[(path[i-1][1])].set_facecolor(col_map[maze[path[i-1][1]]])\n",
    "        if path[i] != 'Eaten' and path[i] != 'Win':\n",
    "            grid.get_celld()[(path[i][0])].set_facecolor(col_map[-2]) # Position of the player\n",
    "            grid.get_celld()[(path[i][1])].set_facecolor(col_map[-1]) # Position of the minotaur\n",
    "        display.display(fig)\n",
    "        time.sleep(1)\n",
    "        display.clear_output(wait = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2242\n",
      "Terminal State:  Eaten\n",
      "did not win nor loose\n",
      "[((0, 0), (6, 5)), ((1, 0), (6, 4)), ((2, 0), (6, 3)), ((3, 0), (6, 2)), ((4, 0), (6, 1)), ((4, 0), (6, 0)), ((5, 0), (6, 1)), 'Eaten']\n"
     ]
    }
   ],
   "source": [
    "# Description of the maze as a numpy array\n",
    "maze = np.array([\n",
    "    [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 1, 1, 1, 1, 1, 0],\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0]])\n",
    "# With the convention 0 = empty cell, 1 = obstacle, 2 = exit of the Maze\n",
    "\n",
    "env = Maze(maze) # Create an environment maze\n",
    "horizon = 20      # TODO: Finite horizon\n",
    "\n",
    "# Solve the MDP problem with dynamic programming\n",
    "V, policy = dynamic_programming(env, horizon)  \n",
    "\n",
    "# Simulate the shortest path starting from position A\n",
    "method = 'DynProg'\n",
    "start  = ((0,0), (6,5))\n",
    "path = env.simulate(start, policy, method)[0]\n",
    "print(path)\n",
    "check_transition_probabilities(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0, 0), (6, 5)), ((1, 0), (6, 4)), ((2, 0), (6, 3)), ((3, 0), (6, 2)), ((4, 0), (6, 1)), ((4, 0), (6, 0)), ((5, 0), (6, 1)), 'Eaten']\n",
      "((0, 0), (6, 5))\n",
      "(0, 0)\n",
      "0\n",
      "{0: '#FFFFFF', 1: '#000000', 2: '#95FD99', -1: '#FFC4CC', -2: '#E8D0FF'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manimate_solution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 46\u001b[0m, in \u001b[0;36manimate_solution\u001b[0;34m(maze, path)\u001b[0m\n\u001b[1;32m     44\u001b[0m     grid\u001b[38;5;241m.\u001b[39mget_celld()[(path[i][\u001b[38;5;241m1\u001b[39m])]\u001b[38;5;241m.\u001b[39mset_facecolor(col_map[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;66;03m# Position of the minotaur\u001b[39;00m\n\u001b[1;32m     45\u001b[0m display\u001b[38;5;241m.\u001b[39mdisplay(fig)\n\u001b[0;32m---> 46\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m display\u001b[38;5;241m.\u001b[39mclear_output(wait \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "animate_solution(maze, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
