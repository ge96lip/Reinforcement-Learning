{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "## b) \n",
    "### Why do we use a replay buffer and target network in DQN?\n",
    "The replay buffer is used to store and reuse past experiences (transitions), defined as tuples $(s, a, r, s{\\prime})$, where:\n",
    "\\begin{itemize}\n",
    "\\item $s$: Current state\n",
    "\\item $a$: Action taken\n",
    "\\item $r$: Reward received\n",
    "\\item $s{\\prime}$: Next state\n",
    "\\end{itemize}\n",
    "\t\n",
    "It is a fact that sequentially collected data from an environment is highly correlated, which can cause the Q-network to overfit to recent experiences. By sampling random minibatches from the replay buffer, the training process becomes more like supervised learning on independent and identically distributed data, improving stability. Furthermore, because each experience is used multiple times for learning, the algorithm makes better use of limited interactions with the environment. Last but not least the replay buffer reduces the variance in gradient updates by averaging over a diverse set of experiences, leading to more stable convergence.\n",
    "\n",
    "A target network is a copy of the Q-network that is used to compute the target $y = r + \\gamma \\max_{a{\\prime}} Q_{\\text{target}}(s{\\prime}, a{\\prime})$ for updating the Q-values. In Q-learning, the target value depends on the same network being trained. This creates a moving target problem, where both the predictions and the targets shift during training, leading to instability. By keeping a separate target network, the target values change more slowly, which improves stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) \n",
    "### Explain the layout of the network that you used; the choice of the optimizer; the parameters that you used $(\\gamma, L, T_E , C, N, \\epsilon)$ and which modification did you implement (if any). Motivate why you made those choices.\n",
    "\n",
    "The final parameter choices for the network were made based on the tips and tricks provided in the assignment sheet. Most importantly the final choices are not tested to be the most efficient, but are the ones that yielded good results over 50 episode average. \n",
    "\n",
    "The optimizer and the clipping_value was choosen as suggested. \n",
    "The network layout was based on the architecture from the exercise session 3. The hidden layers were changed to have 128 neurons each instead of 64 as this gave better results when training. The input and output layer size was adapted to fit the dimensionality of the lunar enviornment. As activation function we only tested ReLU (same as in the exercise session), thus we can't say with confident that this one is the best one across all possibilities. \n",
    "\n",
    "For parameters we oriented ourselves on the tips and tricks again. The discount factor $\\gamma$ we changed from the exercise session's code from 0.99 to 0.95. This value gives more weight to future rewards and does not overly discounting them, as 0.99 does. We also chose a smaller learning rate, as 0.001 did not show stabel training, the second try with $L = 0.0001$ displayed better results. We hypothesize that the smaller learning rate gives more stable updates and prevents overshooting during training.\n",
    "$\\epsilon$ used for the greedy action selection was implemented as in the tutorial and not according to the tip section. As well as the values to initialize $\\epsilon, \\epsilon_{min}, \\epsilon_{decay}$. The key difference between this exponential decay and the exponential decay introduced in the tips section is that in the tip section the rate of decay is calculated to fit within Z episodes, while our decay does not explicitly depend on the total number of episodes. \n",
    "\n",
    "The batch size, $N = 32$, was kept from the exercise session's code. \n",
    "We played around with the experience buffer size, $C$, and found that a small buffer size of $C = 5000$ worked best. \n",
    "We update the target network every 10 epsiodes. This just worked well, thus we decided to only touch tuning this parameter if we have more time in the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) \n",
    "### Plot the total episodic reward and the total number of steps taken per episode during training. What can you say regarding the training process?\n",
    "1. Episodic Reward: Initially, the total episodic reward is negative and highly variable, indicating the agent is struggling to find effective policies and is randomly exploring the environment. Over the episodes, the average reward (orange line) steadily increases, showing that the agent is learning and improving its policy. Toward the later episodes, the reward approaches and stabilizes near positive values, indicating the agent has likely learned an effective strategy for the task.\n",
    "2. Steps Per Episode: Early in training, the agent frequently terminates episodes early, as seen by the low number of steps (blue line). This suggests poor initial policies, likely leading to failures in the task. Over time, the agent takes more steps per episode, which correlates with better performance (as the agent prolongs episodes by achieving task objectives or avoiding failure). The orange line for the average number of steps shows a clear upward trend, further confirming that the agent is learning to interact with the environment more effectively.\n",
    "\n",
    "Summary of Training Process\n",
    "Early Stages:\n",
    "\t•\tHigh exploration, suboptimal policies, and frequent failures.\n",
    "\t•\tLarge variability in rewards and steps.\n",
    "Middle Stages:\n",
    "\t•\tSteady improvement as the agent refines its policy.\n",
    "\t•\tRewards and steps increase on average but with some fluctuations.\n",
    "Later Stages:\n",
    "\t•\tStabilization of performance as the agent transitions to exploitation (less exploration).\n",
    "\t•\tHigher consistency in rewards and steps, indicating convergence to a near-optimal policy.\n",
    "\n",
    "There is a clear correlation between the increasing rewards and the increasing number of steps:\n",
    "\t•\tAs the agent performs better (higher rewards), it is able to survive longer in the environment, reflected in more steps per episode.\n",
    "\t•\tThis is expected in most reinforcement learning tasks where longevity is rewarded.\n",
    "The reward and step counts show variability even in the later stages of training. This variability could be due to:\n",
    "\t•\tStochastic dynamics of the environment.\n",
    "\t•\tEpsilon-greedy exploration still being active, causing occasional suboptimal actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let γ0 be the discount factor you chose that solves the problem. Now choose a discount factor γ1 = 1, and a discount factor γ2 ≪ γ0. Redo the plots for γ1 and γ2 (don’t change the other parameters): what can you say regarding the choice of the discount factor? How does it impact the training process?\n",
    "\n",
    "For $\\gamma 1$ the total reward vs. episodes is lower than for $\\gamma 0$, with the avg. number of steps per episode being higher. The reward per episode (blue line) exhibits greater instability with large fluctuations. Overall one can say that the $\\gamma 0$ if not entirely different in it's trajection exhibits a more stable training. This can be attributed to $\\gamma 1$ giving equal weight to immediate and all future rewards, making the Q-value updates highly sensitive to small changes in future reward estimates.\n",
    "\n",
    "For $\\gamma 2$ the agent achieves higher rewards earlier in training, but the performance plateaus quickly. Furthermore, the number of steps per episode also stabilizes at a lower value compared to $\\gamma 0$. An explanation for this observation is that a low discount factor prioritizes immediate rewards, leading the agent to adopt short-sighted strategies. While this results in faster initial learning, the agent fails to optimize longer-term strategies and often gets stuck in suboptimal behaviors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For your initial choice of the discount factor γ0 investigate the effect of decreasing (or increasing) the number of episodes. Also investigate the effect of reducing (or increasing) the memory size. Document your findings with representative plots.\n",
    "\n",
    "1. Decreasing/ Increasing number of episodes: \n",
    "\n",
    "2. Reducing / Increasing memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) \n",
    " Q(s(y, \\omega), a)  does not significantly increase or decrease as  y  changes, it suggests that the height of the lander does not have a strong influence on the  Q -value at each state. This might imply that the DQN has not effectively learned the importance of being closer to the ground for maximizing long-term rewards.\n",
    "\n",
    " The increase in  Q-values at extreme angles ( \\omega \\to \\pm\\pi ) is counterintuitive. Ideally, upright angles ( \\omega \\approx 0 ) should correspond to higher  Q-values, as they are more likely to lead to successful landings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
