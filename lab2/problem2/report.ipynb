{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b)\n",
    "### Why don’t we use the critic’s target network when updating the actor network? Would it make a difference?\n",
    "\n",
    "Using the current critic ensures that the actor learns from the most recent evaluation of the environment. The target critic is only used for stabilizing the critic updates. Therefore, it would make a difference if we were to use the critic's target network to update the actor network. We hypothesize that using the target critic network would lead to slower convergence because we would then use delayed and potentially outdated estimate of the Q-values, which would lead to slower policy improvements. And the actor would lag in adapting to changes in the critic, leading to less effective policies, especially in environments with rapidly changing dynamics or high variance in rewards.\n",
    "\n",
    "1. The critic network is trained using the Bellman equation calculated by the target critic and target actor.\n",
    "2. The actor network uses the current critic to evaluate the policy. \n",
    "3. The target networks are updated with the soft update rule\n",
    "4. The critic and actor update themselves. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is DDPG off-policy? Is sample complexity an issue for off-policy methods (compared to on-policy ones)?\n",
    "Yes, DDPG is an off-policy algorithm, however every off-policy method can be transformed to be on-policy. One sees that DDPG is an off-policy algorithm because it uses the data stored in the replay buffer, which allows it to reuse past experiences for learning, regardless of the policy that generated them.\n",
    "Sample complexity refers to the number of interactions (state-action-reward tuples) with the environment required to learn an effective policy. Since off-policy methods use a replay buffer, they can repeatedly reuse past experiences for training, effectively reducing the number of environment interactions needed, thus the sample complexity is lower for off-policy methods compared to on-policy methods. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
